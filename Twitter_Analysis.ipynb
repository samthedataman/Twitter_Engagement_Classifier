{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Engagement Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/samsavage/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/samsavage/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import emoji\n",
    "import dateparser\n",
    "import time\n",
    "from scipy import sparse\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import decomposition\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import nltk\n",
    "import pprint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random; random.seed(53)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import \\\n",
    "    StandardScaler, PolynomialFeatures,Normalizer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data  and Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samsavage/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                               partyId  networkAccountId  \\\n",
      "0   1  DF26B924-129B-4A94-A0AE-92153CB13A41        19212009.0   \n",
      "1   2  4100D9EB-F2DA-418B-BA91-4CBDEA5A1AF9       108252113.0   \n",
      "2   3  203B25B6-F5F4-4306-99AA-A22AE0BC0298          890891.0   \n",
      "3   4  230A250C-173A-4EC6-A297-8CA06BDF213A       394216985.0   \n",
      "4   5  ECB8582C-5E9E-4E13-A176-975AA4B98DC7        82652901.0   \n",
      "5   6  0FDDC49F-640A-4A81-B8F6-2CDA629A631C        30278532.0   \n",
      "6   7  613D6C16-1E0B-4954-BDBF-E8B20F711EDA        10005212.0   \n",
      "7   8  391E0B22-79D9-45AD-B7D4-A84818479F25        14749070.0   \n",
      "8   9  819E6F73-0CEB-4B1B-8B91-9F4006294C8D        36072985.0   \n",
      "9  10  934F28DD-9F15-4A74-95B9-8A56A44B413B         5695632.0   \n",
      "\n",
      "                                            contents postDate  engagements  \\\n",
      "0  On this week's @secondlifepod, Hillary chats w...  43:58.0          3.0   \n",
      "1  Humble request to musicians, especially 'new f...  51:50.0        370.0   \n",
      "2  .@dak and his brother embrace after he got pai...  10:00.0       7984.0   \n",
      "3  Spoiler: Game of Thrones' wildfire IRL is dang...  38:28.0         16.0   \n",
      "4  The @BHolidayTheatre presents “12 Angry Men…an...  52:14.0        115.0   \n",
      "5  They're small changes that, in perspective, re...  00:09.0         55.0   \n",
      "6  .@NBCSports wins big at this year’s #TokyoOlym...  23:27.0         10.0   \n",
      "7                          Innovate don’t speculate.  20:09.0        103.0   \n",
      "8     This is a great point: https://t.co/oSQW6awpxu  49:41.0         29.0   \n",
      "9  Pass the tissues, please. https://t.co/HAO3PGX8s4  56:24.0         41.0   \n",
      "\n",
      "   engagementRate   likes  shares   followers  ...  word_count  \\\n",
      "0        0.000001     3.0     0.0   2028283.0  ...          38   \n",
      "1        0.000047   358.0     8.0   7800896.0  ...          43   \n",
      "2        0.000790  7311.0   547.0  10106383.0  ...          13   \n",
      "3        0.000033    13.0     3.0    479135.0  ...           9   \n",
      "4        0.000087    88.0    22.0   1325971.0  ...          41   \n",
      "5        0.000008    46.0     9.0   6597868.0  ...          12   \n",
      "6        0.000070     8.0     1.0    142679.0  ...          15   \n",
      "7        0.000173    81.0    11.0    595749.0  ...           3   \n",
      "8        0.000045    22.0     3.0    644716.0  ...           6   \n",
      "9        0.000006    38.0     3.0   6336681.0  ...           5   \n",
      "\n",
      "  character_count link_count  emoji_count hashtag_count  mention_count  \\\n",
      "0             299          2           15             1              2   \n",
      "1             236          0            4             0              0   \n",
      "2             110          1           14             0              3   \n",
      "3              80          1            8             0              0   \n",
      "4             297          2           13             0              1   \n",
      "5              98          1            5             0              0   \n",
      "6             144          2           15             2              1   \n",
      "7              25          0            1             0              0   \n",
      "8              46          1            5             0              0   \n",
      "9              49          1            4             0              0   \n",
      "\n",
      "   polarity_pos  polarity_neg  polarity_neu  polarity_compound  \n",
      "0         0.000         0.000         1.000             0.0000  \n",
      "1         0.039         0.038         0.923             0.0129  \n",
      "2         0.173         0.000         0.827             0.3182  \n",
      "3         0.000         0.279         0.721            -0.4767  \n",
      "4         0.173         0.128         0.699             0.3182  \n",
      "5         0.000         0.000         1.000             0.0000  \n",
      "6         0.222         0.000         0.778             0.5719  \n",
      "7         0.615         0.000         0.385             0.4939  \n",
      "8         0.506         0.000         0.494             0.6249  \n",
      "9         0.365         0.000         0.635             0.3182  \n",
      "\n",
      "[10 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/samsavage/Downloads/datasampleds_tosend_predict.csv\")\n",
    "#tokenize text\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "#tokenize,stem, and remove stop words from text\n",
    "def cleanText(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "#word count function\n",
    "def word_count(string):\n",
    "    split = string.split()\n",
    "    count = len(split)\n",
    "    return count\n",
    "#count emojis\n",
    "def count_emojis(string):\n",
    "    em_count = len(re.findall(r'[^\\w\\s,.]', string))\n",
    "    return em_count\n",
    "#count hashtags\n",
    "def count_hashtags(string):\n",
    "    em_count = len(re.findall(r'[\\#]', string))\n",
    "    return em_count\n",
    "#count mentions\n",
    "def count_mentions(string):\n",
    "    em_count = len(re.findall(r'[\\@]', string))\n",
    "    return em_count        \n",
    "def text_analyzer(df):\n",
    "    #feature creation{Word Count|character count|emoji count|link count}\n",
    "    df['contents'] = df['contents'].astype('str')\n",
    "    df['word_count'] = df['contents'].apply(word_count)\n",
    "    df['character_count'] = [len(each) for each in df['contents']]\n",
    "    df['link_count'] = df['contents'].str.count(r'(http)')\n",
    "    df['emoji_count'] = df['contents'].apply(count_emojis)\n",
    "    df['hashtag_count'] = df['contents'].apply(count_hashtags)\n",
    "    df['mention_count'] = df['contents'].apply(count_mentions)\n",
    "    #create other features including word count+sentiment based features\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df['polarity_pos'] = df['contents'].apply(lambda x: sid.polarity_scores(x)['pos'])\n",
    "    df['polarity_neg'] = df['contents'].apply(lambda x: sid.polarity_scores(x)['neg'])\n",
    "    df['polarity_neu'] = df['contents'].apply(lambda x: sid.polarity_scores(x)['neu'])\n",
    "    df['polarity_compound'] = df['contents'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "    return df\n",
    "df = text_analyzer(df)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevent features and encode categorical varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           effect1   effect2  word_count  character_count  link_count  \\\n",
      "0      81.77896455  1.309842          38              299           2   \n",
      "1      118.1377431  0.799951          43              236           0   \n",
      "2      174.1372666  1.359366          13              110           1   \n",
      "3      64.05470599  0.900903           9               80           1   \n",
      "4      78.58772967  1.447401          41              297           2   \n",
      "...            ...       ...         ...              ...         ...   \n",
      "99995  70.73702403  1.286623           3               31           1   \n",
      "99996  272.2638861  0.912399          26              161           1   \n",
      "99997  111.4691849  1.421489           9               73           0   \n",
      "99998  112.9585951  0.791044          30              235           1   \n",
      "99999  192.0863126  0.852310          19              136           1   \n",
      "\n",
      "       emoji_count  mention_count  hashtag_count  polarity_pos  polarity_neg  \\\n",
      "0               15              2              1         0.000         0.000   \n",
      "1                4              0              0         0.039         0.038   \n",
      "2               14              3              0         0.173         0.000   \n",
      "3                8              0              0         0.000         0.279   \n",
      "4               13              1              0         0.173         0.128   \n",
      "...            ...            ...            ...           ...           ...   \n",
      "99995            4              0              0         0.615         0.000   \n",
      "99996            8              1              0         0.281         0.000   \n",
      "99997            6              1              2         0.339         0.000   \n",
      "99998            8              0              0         0.214         0.053   \n",
      "99999            4              0              0         0.000         0.000   \n",
      "\n",
      "       ...  polarity_compound   followers  engagements  \\\n",
      "0      ...             0.0000   2028283.0          3.0   \n",
      "1      ...             0.0129   7800896.0        370.0   \n",
      "2      ...             0.3182  10106383.0       7984.0   \n",
      "3      ...            -0.4767    479135.0         16.0   \n",
      "4      ...             0.3182   1325971.0        115.0   \n",
      "...    ...                ...         ...          ...   \n",
      "99995  ...             0.4927    983045.0          NaN   \n",
      "99996  ...             0.8214  34750295.0          NaN   \n",
      "99997  ...             0.5562  13882691.0          NaN   \n",
      "99998  ...             0.6705   8766947.0          NaN   \n",
      "99999  ...             0.0000  54729867.0          NaN   \n",
      "\n",
      "                                                contents mediaType_Video  \\\n",
      "0      On this week's @secondlifepod, Hillary chats w...               0   \n",
      "1      Humble request to musicians, especially 'new f...               0   \n",
      "2      .@dak and his brother embrace after he got pai...               0   \n",
      "3      Spoiler: Game of Thrones' wildfire IRL is dang...               0   \n",
      "4      The @BHolidayTheatre presents “12 Angry Men…an...               0   \n",
      "...                                                  ...             ...   \n",
      "99995                    So good https://t.co/bWMwTuGdeX               0   \n",
      "99996  I've started a @discord account to connect mor...               0   \n",
      "99997  A flawless performance of ‘My Girl’ @victorsol...               0   \n",
      "99998  The Department of Education says it's scrappin...               0   \n",
      "99999  A prototype flying car has completed a test fl...               0   \n",
      "\n",
      "       mediaType_picture  mediaType_text  effect3_A  effect3_C  effect3_D  \n",
      "0                      1               0          0          1          0  \n",
      "1                      0               1          0          1          0  \n",
      "2                      1               0          0          1          0  \n",
      "3                      0               1          0          0          1  \n",
      "4                      0               1          0          1          0  \n",
      "...                  ...             ...        ...        ...        ...  \n",
      "99995                  0               1          0          0          1  \n",
      "99996                  0               1          0          1          0  \n",
      "99997                  0               1          0          0          1  \n",
      "99998                  0               1          0          0          1  \n",
      "99999                  0               1          0          0          1  \n",
      "\n",
      "[100000 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return(res) \n",
    "\n",
    "df = df[['mediaType', 'effect1', 'effect2', 'effect3',\n",
    "       'word_count', 'character_count', 'link_count', 'emoji_count','mention_count',\n",
    "       'hashtag_count', 'polarity_pos', 'polarity_neg',\n",
    "       'polarity_neu', 'polarity_compound','followers','engagements','contents']]\n",
    "df = encode_and_bind(df,'mediaType')\n",
    "df = encode_and_bind(df,'effect3')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Text and combine with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           effect1   effect2  word_count  character_count  link_count  \\\n",
      "0      81.77896455  1.309842          38              299           2   \n",
      "1      118.1377431  0.799951          43              236           0   \n",
      "2      174.1372666  1.359366          13              110           1   \n",
      "3      64.05470599  0.900903           9               80           1   \n",
      "4      78.58772967  1.447401          41              297           2   \n",
      "...            ...       ...         ...              ...         ...   \n",
      "99995  70.73702403  1.286623           3               31           1   \n",
      "99996  272.2638861  0.912399          26              161           1   \n",
      "99997  111.4691849  1.421489           9               73           0   \n",
      "99998  112.9585951  0.791044          30              235           1   \n",
      "99999  192.0863126  0.852310          19              136           1   \n",
      "\n",
      "       emoji_count  mention_count  hashtag_count  polarity_pos  polarity_neg  \\\n",
      "0               15              2              1         0.000         0.000   \n",
      "1                4              0              0         0.039         0.038   \n",
      "2               14              3              0         0.173         0.000   \n",
      "3                8              0              0         0.000         0.279   \n",
      "4               13              1              0         0.173         0.128   \n",
      "...            ...            ...            ...           ...           ...   \n",
      "99995            4              0              0         0.615         0.000   \n",
      "99996            8              1              0         0.281         0.000   \n",
      "99997            6              1              2         0.339         0.000   \n",
      "99998            8              0              0         0.214         0.053   \n",
      "99999            4              0              0         0.000         0.000   \n",
      "\n",
      "       ...  222933  222934  222935  222936 222937  222938  222939  222940  \\\n",
      "0      ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "1      ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "2      ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "3      ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "4      ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "...    ...     ...     ...     ...     ...    ...     ...     ...     ...   \n",
      "99995  ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "99996  ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "99997  ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "99998  ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "99999  ...     0.0     0.0     0.0     0.0    0.0     0.0     0.0     0.0   \n",
      "\n",
      "       222941  222942  \n",
      "0         0.0     0.0  \n",
      "1         0.0     0.0  \n",
      "2         0.0     0.0  \n",
      "3         0.0     0.0  \n",
      "4         0.0     0.0  \n",
      "...       ...     ...  \n",
      "99995     0.0     0.0  \n",
      "99996     0.0     0.0  \n",
      "99997     0.0     0.0  \n",
      "99998     0.0     0.0  \n",
      "99999     0.0     0.0  \n",
      "\n",
      "[100000 rows x 222964 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=cleanText)\n",
    "tfidf_vect_fit = tfidf_vect.fit_transform(df['contents'].fillna(''))\n",
    "tfidf_vect_fit_df = pd.DataFrame.sparse.from_spmatrix(tfidf_vect_fit)\n",
    "tfidf_vect_fit_df\n",
    "df = pd.concat([df,tfidf_vect_fit_df], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Validation,Test,Training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000,)\n",
      "(90000, 222962)\n"
     ]
    }
   ],
   "source": [
    "validation = df.iloc[90001:,]\n",
    "train = df.iloc[0:90000:,]\n",
    "y = train['engagements'].fillna(0)\n",
    "X = train.drop(columns={'engagements','contents'})\n",
    "print(y.shape)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samsavage/mambaforge/envs/grailed_scraper/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/samsavage/mambaforge/envs/grailed_scraper/lib/python3.9/site-packages/sklearn/utils/validation.py:624: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = LinearRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(predictions)\n",
    "print(mean_squared_error(y_test,predictions,squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'neg_mean_squared_error'\n",
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "models.append(('Ridge', Ridge()))\n",
    "models.append(('Lasso', Lasso()))\n",
    "models.append(('Elastic', ElasticNet()))\n",
    "models.append(('GBR', GradientBoostingRegressor()))\n",
    "models.append(('SVR', SVR()))\n",
    "models.append(('Decision', DecisionTreeRegressor()))\n",
    "models.append(('MLP', MLPRegressor()))\n",
    "seed = 5\n",
    "\n",
    "##################################################\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "score = 'neg_mean_squared_error'\n",
    "# store preds\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "dwPreds = []\n",
    "for name, model in models:\n",
    " kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n",
    " # store the metrics\n",
    " cv_results = cross_val_score(model, X_train,y_train, cv=kfold, scoring = score)\n",
    " results.append(cv_results)\n",
    " names.append(name)\n",
    " msg = \"%s: %f (%f)\" % (name, abs(cv_results.mean()), cv_results.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algo Comparision Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compare Algorithms\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m22\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      3\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlgorithm Comparison\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m111\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure(figsize=(22,10))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Blank Engagment Records from Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d224f8e5b8614e2539c4d9cd5f2211979b2e2ffb59ab2a3bcdc2d2f7b2d85003"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('grailed_scraper': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
